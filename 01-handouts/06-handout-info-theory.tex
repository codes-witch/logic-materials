\PassOptionsToPackage{table}{xcolor}
\documentclass[nobib,nofonts]{tufte-handout}

%\geometry{showframe} % display margins for debugging page layout

%%% MF additions
% \usepackage[table]{xcolor}
\usepackage[nographicx, nohyperref, nosubcaption, nogb4e, nobiblatex]{../99-auxiliary-files/00-mypackages}
\usepackage{../99-auxiliary-files/00-mycommands}
\usepackage{../99-auxiliary-files/00-myenvironments}

\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{tikz-qtree}
\usepackage{subcaption}

\usepackage{pgfplots}
% externalize PGF plots
% \usepgfplotslibrary{external}
% \tikzexternalize

% \titleformat{\section}
% {\large\bfshape}{\thesection}{1em}{}

\setcounter{secnumdepth}{5}
\renewcommand\thesection{\arabic{section}}

% this length controls tha hanging indent for titles
% change the value according to your needs
\newlength\titleindent
\setlength\titleindent{0.7cm}

\pretocmd{\paragraph}{\stepcounter{subsection}}{}{}
\pretocmd{\subparagraph}{\stepcounter{subsubsection}}{}{}

\titleformat{\chapter}[block]
  {\normalfont\huge\bfseries}{}{0pt}{\hspace*{-\titleindent}}

\titleformat{\section}
  {\normalfont\Large\itshape}{\llap{\parbox{\titleindent}{\thesection\hfill}}}{0em}{}

\titleformat{\subsection}
  {\normalfont\itshape}{\llap{\parbox{\titleindent}{\thesubsection\hfill}}}{0em}{}

\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape}{\llap{\parbox{\titleindent}{\thesubsubsection}}}{0em}{}

\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\itshape}{}{-0.7cm}{}[\xspace \ \ \ \ ]

\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize}{\llap{\parbox{\titleindent}{\thesubsubsection\hfill}}}{0em}{}

\titlespacing*{\chapter}{0pt}{0pt}{20pt}
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{0em}
\titlespacing*{\subparagraph}{0pt}{3.25ex plus 1ex minus .2ex}{0em}

\DefineNamedColor{named}{mygray2}{cmyk}{0.55,0.25,0.25,0.25}
\newcommand{\mygray}[1]{\textcolor{mygray2}{#1}}

%%% Tufte style
\usepackage{graphicx} % allow embedded images
  \setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
  \graphicspath{{graphics/}} % set of paths to search for images

\usepackage{fancyvrb} % extended verbatim environments
  \fvset{fontsize=\normalsize}% default font size for fancy-verbatim environments

% Standardize command font styles and environments
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment

\newcommand{\proplog}{\acro{PropLog}}
\newcommand{\EFSQ}{\ensuremath{\mathit{EFSQ}}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage[sc,osf]{mathpazo}
% \linespread{1.05}



\title{A primer on information theory (for the subjectively perplexed) }

\author[M.~Franke]{Michael Franke}

\date{} % without \date command, current date is supplied

\begin{document}

\maketitle

\begin{abstract}
\noindent
Surprisal, entropy (joint, conditional, cross), Kullback-Leibler divergence, mutual information.
\end{abstract}

\noindent The goal of this primer on information theory is to introduce the most salient notions of information theory relevant to common applications in fields like machine learning, computational linguistics or theoretical linguistics.
Rather than appealing to deeper mathematical results (such as related to noisy communication channels and efficient coding), this primer explains and motivates the relevant notions from the perspective of (subjective) beliefs.

\section{A measure of information gained}

At the heart of information theory lies a numerical measure of the amount of information learned, so-called \emph{information content} or \emph{surprisal}.
Let us motivate this measure based on three intuitive desiderata.

\subsection{Motivation}

Suppose that Jones and Smith are uncertain about the weather tomorrow at noon.
There are only three possible states of the weather $X = \set{\text{sunny}, \text{misty}, \text{rainy}}$.
Jones' and Smith's subjective beliefs are given in Table~\ref{tab:beliefs-weather}.

\begin{table}
  \centering
  \begin{tabular}{lccc}
    & sunny & cloudy & rainy \\ \midrule
    Jones' beliefs  & 0.6   & 0.2    & 0.2   \\
    Smith's beliefs & 0.1   & 0.2    & 0.7   \\
  \end{tabular}

  \caption{Subjective beliefs about the weather.}
  \label{tab:beliefs-weather}
\end{table}

The next day, Jones and Smith both observe that it is in fact sunny.
Who learns more? Who is more surprised by this turn of events? ---
Intuitively, since Jones had expected sunny weather, he gains less information than Smith, who had though that this event is rather unlikely.
In sum, a measure of the ``information learned'' should be sensitive to an agent's prior expectations $P \in D(X)$.
We therefore aim to define a numerical measure $I_{P}(x)$, which assigns a number representing the information gained by observing $x \in X$, given prior beliefs $P \in \Delta(X)$.

There are three further desiderata on the measure $I_{P}(x)$:
\begin{enumerate}
  \item \textbf{Lower bound.} If an agent is already maximally certain that $x$ would occur, so that $P(x) = 0$, the agent learns nothing from observing $x$, so that $I_{P}(x)=0$.
  \item \textbf{Monotonicity.} The lower the prior probability of $P(x)$ the more information is gained from (the more surprised the agent is by) observing an event $x \in X$.
  \item \textbf{Additivity.} If an agent observes two independent events $x_{1}, x_{2} \in X$, the total information gained should be the sum of the information gained from each individual information: $I_{P}(x_{1} \& x_{2}) = I_{P}(x_{1}) I_{P}(x_{2})$.
\end{enumerate}
The family of functions that satisfies these desiderata is that of negative logarithms (up to a scaling factor).
Most frequently, the logarithm to base 2 is used.

\subsection{Information content (surprisal)}

Let $P\in \Delta(X)$ be a probability distribution over (finite) set $X$.
For event $x \in X$, the \emph{information content} $I_{P}(x)$ of $x$ (a.k.a.~\emph{surprisal} of $x$) under random variable $X$ is defined as:
\begin{align*}
  I_{P}(x) = - \log_{2} P(x)
\end{align*}
Intuitively speaking, the information content $I_{P}(x)$ is a measure of how surprised an agent with beliefs $P$ is (alternatively: how much the agent learns) when they observe $x$.


\begin{figure}
  \centering
%   \begin{tikzpicture}
% \begin{axis}[
%     width      = 8cm,
%     height     = 6cm,
%     axis lines = left,
%     xlabel     = \(x\),
%     ylabel     = {\(\log_{2} x\)},
% ]
% %Below the red parabola is defined
% \addplot [
%     domain=-0:4,
%     samples=50,
%     very thick,
%     color=mygray2,
%     ]
%     {log2(x)};

% \addplot[
%     mark=none,
%     dotted,
%     domain=-0:4,
%     black,
%     samples=2,
%     ] {0};

% \end{axis}
% \end{tikzpicture}
% \hfill
\begin{tikzpicture}
\begin{axis}[
    width      = 8cm,
    height     = 6cm,
    axis lines = left,
    xlabel = \(P(x)\),
    ylabel = {\(I_{P}(x)\)},
]
%Below the red parabola is defined
\addplot [
    domain=-0:1,
    samples=50,
    very thick,
    color=mygray2,
    ]
    {-log2(x)};

\addplot[
    mark=none,
    dotted,
    domain=-0:1,
    black,
    samples=2,
    ] {0};

\end{axis}
\end{tikzpicture}

\caption{Information content / surprisal (to base 2) for an event $x$ as a function of its prior probability $P(x)$.}
\label{fig:log-surprisal}
\end{figure}

Notice that an agent who assigns $P(x) = 0$ to some event, and sees $x$ happening, is infinitely perplexed (has their mind blown).

\section{Measures of expected information content}

The basic measures used in applications of information theory fall in one of two categories.
For one there are \emph{measures of expected information content}.
These measures follow the general format:
\begin{align*}
  \sum_{x \in X} \text{Probability of } x \ \times \text{Info-content of } x
\end{align*}
and are discussed in this section.
Different measures of this format differ in what kind of distributions are used to define the probability of $x$ and the information content of $x$.

The other class of information-theoretic measures commonly used in applications are \emph{measures of expected difference in information content}.
These measures instantiating are dealt with in Section~\ref{sec:meas-expect-diff}, and they instantiate the template:
\begin{align*}
  \sum_{x \in X} \text{Probability of } x \ \times (\text{ Inf.cont } x \text{ wrt. Q } - \text{ Inf.cont. } x \text{ wrt. P })
\end{align*}
where information content is measured based on two different distributions $P$ and $Q$.

\subsection{Entropy}

Let $P\in \Delta(X)$ be a probability distribution over (finite) set $X$.
The \emph{entropy} $\mathcal{H}(P)$ of probability distribution $P$ is the expected information content under the assumption that the true distribution is $P$:\sidenote{
  Here and below, writing ``true distribution'' or similar formulations does not necessarily entail a strong commitment to actual truth.
  It is shorthand for more careful but cumbersome language like ``the distribution used as a reference or baseline which we assume to be true or treat as-if true.'' }

\begin{align*}
  \mathcal{H}(P) & = \sum_{x \in X} P(x) \ I_{P}(x) \\
                 & = - \sum_{x \in X} P(x) \ \log_{2} P(x)
\end{align*}
Intuitively speaking, the entropy $\mathcal{H}(P)$ measures the expected (or average) surprisal of an agent whose beliefs are $P$ when the true distribution is $P$.
Entropy can also be interpreted as a measure of uncertainty: the higher the entropy of $P$ the more uncertain an agent an agent with beliefs $P$ is about $X$.


\begin{example}
  The entropy of Jones' beliefs in Table~\ref{tab:beliefs-weather} is:
  \begin{align*}
    \mathcal{H}(P_{J})
 & = - \sum_{x \in \set{\text{sunny}, \text{cloudy}, \text{rainy}}} P_{J}(x) \ \log_{2} P_{J}(x) \\
 & = - (0.6 \times \log_{2} 0.6
   +  0.2 \times \log_{2} 0.2
   +  0.2 \times \log_{2} 0.2) \\
 & \approx - (0.6 \times -0.74 + 0.4 \times -2.33) \\
 & \approx 1.37
  \end{align*}
  A similar calculation for Smith's beliefs yields: $\mathcal{H}(P_{S}) \approx 1.16$.
  Smith is slightly less uncertain than Jones about the state of the weather.
\end{example}

\subsection{Cross entropy}

Let $P, Q \in \Delta(X)$ be probability distributions over (finite) set $X$.
The \emph{cross entropy} $\mathcal{H}(P,Q)$ of probability distributions $P$ and $Q$ measures the expectation of information content given $Q$ from the point of view of (assumed true) distribution $P$:
\begin{align*}
  \mathcal{H}(P,Q) & = \sum_{x \in X} P(x) \ I_{Q}(x) \\
                   & = - \sum_{x \in X} P(x) \ \log Q(x)
\end{align*}
Intuitively speaking, the cross entropy $\mathcal{H}(P,Q)$ measures the expected (or average) surprisal of an agent whose beliefs are $Q$ when the true distribution is $P$.

\begin{example}
  Suppose that Jones is exactly right.
  The beliefs $P_{J}$ capture the precise probability of the weather in the limit (when not taking the weather conditions on the previous days into account).
  Smith's beliefs are therefore not quite in line with reality.
  The cross entropy $\mathcal{H}(P_{J}, P_{S})$ then measures Smith average surprisal by taking the real-world frequencies to form the expectation and by using Smith's beliefs to define the surprisal:
  \begin{align*}
    \mathcal{H}(P_{J}, P_{S}) & = - \sum_{x \in X} P_{J}(x) \ \log P_{S}(x) \\
    & = -( 0.6 * \log_{2} 0.1  +
    0.2 * \log_{2} 0.2  +
      0.2 * \log_{2} 0.7  ) \\
    & \approx 2.56
  \end{align*}
\end{example}


\subsection{Joint entropy}

Joint entropy is entropy for joint probability distributions.\sidenote{
  Here, we only look at structured spaces with two dimensions. Joint entropy can be generalized to further dimensions in analogous manner.}
Let $R \in \Delta(X \times Y)$ by a joint probability distribution over the set of all pairs in the structured event space $X \times Y$, and let $P \in \Delta(X)$ and $Q \in \Delta(Y)$ be the \emph{marginal distributions} over $X$ and $Y$ respectively.\sidenote{
  So, $P(x) = \sum_{y \in Y} R(x,y)$ and similarly for $Q$.
}
The \emph{joint entropy} $\mathcal{H}(P,Q)$ of $P$ and $Q$ is defined as:\sidenote{
  A usual definition of joint entropy is in terms of random variables $X$ and $Y$ and reads as $\mathcal{H}(X, Y) = - \sum_{x \in X} \sum_{y\in Y} P(x,y) \log_{2} P(x,y)$, where it is implicitly assumed that there is an encompassing joint probability distribution $P(x,y)$ over pairs of numbers. Giving the definition in the way we do here makes clear how joint entropy is really nothing special at all, except when you muffle the joint distribution through intransparency with random variable notation.
}
\begin{align*}
  \mathcal{H}(P, Q) & = - \sum_{x \in X} \sum_{y\in Y} R(x,y) \log_{2} R(x,y)
\end{align*}
which is just the entropy of the joint probability distribution $R$:
\begin{align*}
  \mathcal{H}(P, Q) = \mathcal{H}(R) = - \sum_{z \in X \times Y} R(z) \log_{2} R(z)
\end{align*}

\begin{example}

Clark also has beliefs about the weather, but theirs are a joint belief $R \in \Delta(X \times Y)$ about the condition of the weather ($X \in \set{\text{sunny}, \text{cloudy}, \text{rainy}} $) and whether the swallow fly high or low in the evening ($Y \in \set{\text{high}, \text{low}}$), as shown in Table~\ref{tab:joint-prob-Clark}.

  \begin{table}

    \begin{tabular}{lcccr}
           & sunny                 & cloudy                & rainy                 & $\sum$ rows \\ \midrule
      high & $.6 \times .4 = .24$ & $.2 \times .4 = .08$ & $.2 \times .4 = .08$ & .4 \\
      low  & $.1 \times .6 = .06$ & $.2 \times .6 = .12$ & $.7 \times .6 = .42$ & .6 \\ \midrule
      $\sum$ columns & .3 & .2 & .6
    \end{tabular}

  \caption{Clark's joint probabilistic belief about the flying of swallows in the evening and the weather condition the next day..}
  \label{tab:joint-prob-Clark}

\end{table}

\medskip

  The joint entropy of Clark's beliefs can be calculated as:
  \begin{align*}
    \mathcal{H}(R) & = - \sum_{x \in \set{\text{sunny}, \text{cloudy}, \text{rainy}}} \sum_{y \in \set{\text{high}, \text{low}}} R(x,y) \ \log_{2} R(x,y) \\
                   & = - (.24 \log_{2} .24  +
                     .08 \log_{2} .08  +
                     .08 \log_{2} .08  +
                     .06 \log_{2} .06  +
                     \dots
                     ) \\
    & \approx 2.22
  \end{align*}


\end{example}

\subsection{Conditional entropy}

Let $R \in \Delta(X \times Y)$ by a joint probability distribution over the set of all pairs in $X \times Y$, and let $P \in \Delta(X)$ and $Q \in \Delta(Y)$ be the \emph{marginal distributions} over $X$ and $Y$ respectively.
The conditional entropy of $Q$ given $P$ is the expected surprisal of observing $y$ after having updated beliefs with the corresponding $x$:
\begin{align*}
  \mathcal{H}(P \mid Q) & = - \sum_{\tuple{x,y} \in X \times Y} R(x,y) \log_{2} R(x \mid y) \\
                        & = - \sum_{\tuple{x,y} \in X \times Y} R(x \mid y) Q(y) \log_{2} R(x \mid y) \\
                        & = - \underbrace{\sum_{y \in Y} Q(y)}_{\text{prob.~of $y$}}
                          \underbrace{\sum_{x \in X} R(x \mid y) \log_{2} R(x \mid y)}_{\text{entropy of $R(\cdot \mid y)$}}
\end{align*}
We can also think about conditional entropy as the average uncertainty of an agent about dimension $Y$ after observing dimension $X$.

\begin{example}
  Suppose that Clark observes the swallows every night and then makes a weather prediction.
  If $P \in X$ captures Clark's marginal beliefs about the weather and $Q \in Y$ those about the swallows flying, then, from Clark's subjective point of view, the expected surprisal of their weather predictions after having observed the swallows flying is given by the conditional entropy:
  \begin{align*}
    \mathcal{H}(P \mid Q) & = - \sum_{y \in Y} Q(y)
                          \sum_{x \in X} R(x \mid y) \log_{2} R(x \mid y) \\
                          & = Q(\text{high}) \ [ R(\text{sunny} \mid \text{high}) \ \log_{2} R(\text{sunny} \mid \text{high})  \\
                          & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + R(\text{cloudy} \mid \text{high}) \ \log_{2} R(\text{cloudy} \mid \text{high}) \\
                          & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + R(\text{rainy} \mid \text{high}) \ \log_{2} R(\text{rainy } \mid \text{high}) ] + \\
                           &\ \ \  \ \ \ Q(\text{low}) \ [ R(\text{sunny} \mid \text{low}) \ \log_{2} R(\text{sunny} \mid \text{low})  \\
                          & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + R(\text{cloudy} \mid \text{low}) \ \log_{2} R(\text{cloudy} \mid \text{low}) \\
                          & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + R(\text{rainy} \mid \text{low}) \ \log_{2} R(\text{rainy } \mid \text{low}) ] \\
                          & = 0.4 [0.6 \log_{2} 0.6 +  0.2 \log_{2} 0.2 + 0.2 \log_{2} 0.2] + \\
    & \ \ \ \ 0.6 [0.1 \log_{2} 0.1 +  0.2 \log_{2} 0.2 + 0.7 \log_{2} 0.7]
  \end{align*}
\end{example}

\section{Measure of expected difference in information content }
\label{sec:meas-expect-diff}


\subsection{Kullback-Leibler divergence (relative entropy)}

The \emph{Kullback-Leibler (KL) divergence} (also known as \emph{relative entropy}) measures the expected difference in information content between the distribution $Q \in \Delta(X)$ and the true distribution $P \in \Delta(X)$:
\begin{align*}
  D_{KL}(P \,||\, Q) & = \sum_{x \in X} P(x) \ \left ( I_{Q}(x) - I_{P}(x) \right ) \\
  & = \sum_{x \in X} P(x) \ \log  \frac{P(x)}{Q(x)}
\end{align*}
Intuitively speaking, the KL-divergence $D_{KL}(P \,||\, Q)$ measures how much more surprised an agent is, on average, when they hold beliefs described by $Q$ instead of the true distribution $P$.

KL-divergence $D_{KL}(P \,||\, Q)$ can be equivalently written in terms of the entropy $\mathcal{H}(P)$ of $P$ and the cross entropy $\mathcal{H}(P,Q)$:
\begin{align*}
  D_{KL}(P \,||\, Q) = \mathcal{H}(P,Q) - \mathcal{H}(P)
\end{align*}
% include this as exercise

\begin{itemize}
  \item examples
  \item not a metric
\end{itemize}


\section{Mutual information}

Let $R \in \Delta(X \times Y)$ by a joint probability distribution over the set of all pairs in $X \times Y$, and let $P \in \Delta(X)$ and $Q \in \Delta(Y)$ be the \emph{marginal distributions} over $X$ and $Y$ respectively.
The \emph{mutual information} $I(P,Q)$ of $P$ and $Q$ is the expected excess surprisal if the dimensions $X$ and $Y$ are assumed to be stochastically independent:
\begin{align*}
  I(X,Y) & = \sum_{\tuple{x,y} \in X \times Y} R(x,y) \log \frac{R(x,y)}{P(x) \ Q(y)}
\end{align*}
We can write this more intelligibly in terms of the Kullback-Leibler divergence between true distribution $R$ and the distribution $S \in \Delta(X \times Y)$ which is derived from $P$ and $Q$ by assuming that the dimensions $X$ and $Y$ are stochastically independent, so that $S(x,y) = P(x) \ Q(x)$:
\begin{align*}
  I(X,Y) & =  D_{KL}(R \,||\, S) = \sum_{\tuple{x,y} \in X \times Y} R(x,y) \log \frac{R(x,y)}{S(x,y)}
\end{align*}
Intuitively, we may think of mutual information as a measure of how much more (needlessly) surprised an agent is who believes $X$ and $Y$ are stochastically independent (while having correct beliefs of then marginal distributions).

\bigskip
\noindent \colorbox{mygray}{\centering
  \begin{minipage}{1.0\textwidth}


    \begin{exercise}
      Roberts and Carpenter hold the following beliefs about the weather:

      \begin{center}
        \begin{tabular}{lcccc}
                              & sunny & cloudy & rainy & storm \\ \midrule
          Roberts beliefs     & 0.1   & 0.6    & 0.2   & 0.1 \\
          Carpenter's beliefs & 0.4   & 0.1    & 0.3   & 0.2 \\
        \end{tabular}
      \end{center}

        Calculate the entropy of their beliefs. Who is more uncertain?
    \end{exercise}

    \begin{exercise}
      Proof that indeed, as claimed above, $D_{KL}(P \,||\, Q) = \mathcal{H}(P,Q) - \mathcal{H}(P)$
    \end{exercise}

  \end{minipage}
}



\end{document}

Applications (Gerhard):

entropy as measure of language complexity (e.g., in morphological markers)
mutual information as measure of degree of relatedness between languages
