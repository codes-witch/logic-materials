\PassOptionsToPackage{table}{xcolor}
\documentclass[nobib,nofonts]{tufte-handout}

%\geometry{showframe} % display margins for debugging page layout

%%% MF additions
% \usepackage[table]{xcolor}
\usepackage[nographicx, nohyperref, nosubcaption, nogb4e, nobiblatex]{../99-auxiliary-files/00-mypackages}
\usepackage{../99-auxiliary-files/00-mycommands}
\usepackage{../99-auxiliary-files/00-myenvironments}

\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{tikz-qtree}
\usepackage{subcaption}

\usepackage{pgfplots}
% externalize PGF plots
% \usepgfplotslibrary{external}
% \tikzexternalize

% \titleformat{\section}
% {\large\bfshape}{\thesection}{1em}{}

\setcounter{secnumdepth}{5}
\renewcommand\thesection{\arabic{section}}

% this length controls tha hanging indent for titles
% change the value according to your needs
\newlength\titleindent
\setlength\titleindent{0.7cm}

\pretocmd{\paragraph}{\stepcounter{subsection}}{}{}
\pretocmd{\subparagraph}{\stepcounter{subsubsection}}{}{}

\titleformat{\chapter}[block]
  {\normalfont\huge\bfseries}{}{0pt}{\hspace*{-\titleindent}}

\titleformat{\section}
  {\normalfont\Large\itshape}{\llap{\parbox{\titleindent}{\thesection\hfill}}}{0em}{}

\titleformat{\subsection}
  {\normalfont\itshape}{\llap{\parbox{\titleindent}{\thesubsection\hfill}}}{0em}{}

\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape}{\llap{\parbox{\titleindent}{\thesubsubsection}}}{0em}{}

\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\itshape}{}{-0.7cm}{}[\xspace \ \ \ \ ]

\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize}{\llap{\parbox{\titleindent}{\thesubsubsection\hfill}}}{0em}{}

\titlespacing*{\chapter}{0pt}{0pt}{20pt}
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{0em}
\titlespacing*{\subparagraph}{0pt}{3.25ex plus 1ex minus .2ex}{0em}

\DefineNamedColor{named}{mygray2}{cmyk}{0.55,0.25,0.25,0.25}
\newcommand{\mygray}[1]{\textcolor{mygray2}{#1}}

%%% Tufte style
\usepackage{graphicx} % allow embedded images
  \setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
  \graphicspath{{graphics/}} % set of paths to search for images

\usepackage{fancyvrb} % extended verbatim environments
  \fvset{fontsize=\normalsize}% default font size for fancy-verbatim environments

% Standardize command font styles and environments
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment

\newcommand{\proplog}{\acro{PropLog}}
\newcommand{\EFSQ}{\ensuremath{\mathit{EFSQ}}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage[sc,osf]{mathpazo}
% \linespread{1.05}



\title{Basics of information theory}

\author[M.~Franke]{Michael Franke}

\date{} % without \date command, current date is supplied

\begin{document}

\maketitle

\begin{abstract}
\noindent
Surprisal, entropy, Kullback-Leibler divergence, mutual information.
\end{abstract}

The goal of this primer on information theory is to introduce the most salient notions of information theory relevant to common applications in fields like machine learning, computational linguistics or theoretical linguistics.
Rather than appealing to deeper mathematical results (such as related to noisy communication channels and efficient coding), this primer explains and motivates the relevant notions from the perspective of (subjective) beliefs.

\begin{figure}
  \centering
  \begin{tikzpicture}
\begin{axis}[
    width      = 8cm,
    height     = 6cm,
    axis lines = left,
    xlabel     = \(x\),
    ylabel     = {\(\log_{2} x\)},
]
%Below the red parabola is defined
\addplot [
    domain=-0:4,
    samples=50,
    very thick,
    color=mygray2,
    ]
    {log2(x)};

\addplot[
    mark=none,
    dotted,
    domain=-0:4,
    black,
    samples=2,
    ] {0};

\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    width      = 8cm,
    height     = 6cm,
    axis lines = left,
    xlabel = \(P(x)\),
    ylabel = {\(\text{Surprisal}( x ) \)},
]
%Below the red parabola is defined
\addplot [
    domain=-0:1,
    samples=50,
    very thick,
    color=mygray2,
    ]
    {-log2(x)};

\addplot[
    mark=none,
    dotted,
    domain=-0:1,
    black,
    samples=2,
    ] {0};

\end{axis}
\end{tikzpicture}

\caption{Logarithm and surprisal (to base 2).}
\label{fig:log-surprisal}
\end{figure}

\section{Information content (surprisal)}

Let $P\in \Delta(X)$ be a probability distribution over (finite) set $X$.
For event $x \in \mathcal{X}$, the \emph{information content} $I_{X}(x)$ of $x$ (a.k.a.~\emph{surprisal} of $x$) under random variable $X$ is defined as:
\begin{align*}
  I_{P}(x) = - \log_{2} P(x)
\end{align*}
Intuitively speaking, the information content $I_{P}(x)$ is a measure of how surprised an agent with beliefs $P$ is (alternatively: how much the agent learns) when they observe $x$.

\section{Entropy}

Let $P\in \Delta(X)$ be a probability distribution over (finite) set $X$.
The \emph{entropy} $\mathcal{H}(P)$ of probability distribution $P$ is the expected information content under the assumption that the true distribution is $P$:\sidenote{Here and below, writing ``true distribution'' or similar formulations does not necessarily entail a strong commitment to actual truth. It is shorthand for more careful but cumbersome language like ``the distribution used as a reference or baseline which we assume to be true or treat as-if true.'' }
\begin{align*}
  \mathcal{H}(P) & = \sum_{x \in \mathcal{X}} P(x) \ I_{P}(x) \\
                 & = - \sum_{x \in \mathcal{X}} P(x) \ \log P(x)
\end{align*}
Intuitively speaking, the entropy $\mathcal{H}(P)$ measures the expected (or average) surprisal of an agent whose beliefs are $P$ when the true distribution is $P$.

\begin{itemize}
  \item example
  \item joint and conditional entropy
\end{itemize}

\section{Cross entropy}

Let $P, Q \in \Delta(X)$ be probability distributions over (finite) set $X$.
The \emph{cross entropy} $\mathcal{H}(P,Q)$ of probability distributions $P$ and $Q$ measures the expectation of information content given $Q$ from the point of view of (assumed true) distribution $P$:
\begin{align*}
  \mathcal{H}(P,Y) & = \sum_{x \in \mathcal{X}} P(x) \ I_{Q}(x) \\
                   & = - \sum_{x \in \mathcal{X}} P(x) \ \log Q(x)
\end{align*}
Intuitively speaking, the cross entropy $\mathcal{H}(P,Q)$ measures the expected (or average) surprisal of an agent whose beliefs are $Q$ when the true distribution is $P$.

\section{Kullback-Leibler divergence (relative entropy)}

The \emph{Kullback-Leibler (KL) divergence} (also known as \emph{relative entropy}) measures the expected (or average) difference in information content between the distribution $Q \in \Delta(X)$ and the true distribution $P \in \Delta(X)$:
\begin{align*}
  D_{KL}(P || Q) & = \sum_{x \in \mathcal{X}} P(x) \ \left ( I_{Q}(x) - I_{P}(x) \right ) \\
  & = \sum_{x \in \mathcal{X}} P(x) \ \log  \frac{P(x)}{Q(x)}
\end{align*}
Intuitively speaking, the KL-divergence $D_{KL}(P || Q)$ measures how much more surprised an agent is, on average, when they hold beliefs described by $Q$ instead of the true distribution $P$.

KL-divergence $D_{KL}(P || Q)$ can be equivalently written in terms of the entropy $\mathcal{H}(P)$ of $P$ and the cross entropy $\mathcal{H}(P,Q)$:
\begin{align*}
  D_{KL}(P || Q) = \mathcal{H}(P,Q) - \mathcal{H}(P)
\end{align*}
% include this as exercise

\begin{itemize}
  \item examples
  \item not a metric
\end{itemize}


\section{Mutual information}

\begin{align*}
  I(X;Y) & = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(Z)
\end{align*}

\end{document}

Applications (Gerhard):

entropy as measure of language complexity (e.g., in morphological markers)
mutual information as measure of degree of relatedness between languages
