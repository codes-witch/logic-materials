\PassOptionsToPackage{table}{xcolor}
\documentclass[nobib,nofonts]{tufte-handout}

%\geometry{showframe} % display margins for debugging page layout

%%% MF additions
% \usepackage[table]{xcolor}
\usepackage[nographicx, nohyperref, nosubcaption, nogb4e, nobiblatex]{../99-auxiliary-files/00-mypackages}
\usepackage{../99-auxiliary-files/00-mycommands}
\usepackage{../99-auxiliary-files/00-myenvironments}

\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{tikz-qtree}
\usepackage{subcaption}

\usepackage{pgfplots}
% externalize PGF plots
% \usepgfplotslibrary{external}
% \tikzexternalize

% \titleformat{\section}
% {\large\bfshape}{\thesection}{1em}{}

\setcounter{secnumdepth}{5}
\renewcommand\thesection{\arabic{section}}

% this length controls tha hanging indent for titles
% change the value according to your needs
\newlength\titleindent
\setlength\titleindent{0.7cm}

\pretocmd{\paragraph}{\stepcounter{subsection}}{}{}
\pretocmd{\subparagraph}{\stepcounter{subsubsection}}{}{}

\titleformat{\chapter}[block]
  {\normalfont\huge\bfseries}{}{0pt}{\hspace*{-\titleindent}}

\titleformat{\section}
  {\normalfont\Large\itshape}{\llap{\parbox{\titleindent}{\thesection\hfill}}}{0em}{}

\titleformat{\subsection}
  {\normalfont\itshape}{\llap{\parbox{\titleindent}{\thesubsection\hfill}}}{0em}{}

\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape}{\llap{\parbox{\titleindent}{\thesubsubsection}}}{0em}{}

\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\itshape}{}{-0.7cm}{}[\xspace \ \ \ \ ]

\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize}{\llap{\parbox{\titleindent}{\thesubsubsection\hfill}}}{0em}{}

\titlespacing*{\chapter}{0pt}{0pt}{20pt}
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{0em}
\titlespacing*{\subparagraph}{0pt}{3.25ex plus 1ex minus .2ex}{0em}

\DefineNamedColor{named}{mygray2}{cmyk}{0.55,0.25,0.25,0.25}
\newcommand{\mygray}[1]{\textcolor{mygray2}{#1}}

%%% Tufte style
\usepackage{graphicx} % allow embedded images
  \setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
  \graphicspath{{graphics/}} % set of paths to search for images

\usepackage{fancyvrb} % extended verbatim environments
  \fvset{fontsize=\normalsize}% default font size for fancy-verbatim environments

% Standardize command font styles and environments
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment

\newcommand{\proplog}{\acro{PropLog}}
\newcommand{\EFSQ}{\ensuremath{\mathit{EFSQ}}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage[sc,osf]{mathpazo}
% \linespread{1.05}



\title{Basics of information theory}

\author[M.~Franke]{Michael Franke}

\date{} % without \date command, current date is supplied

\begin{document}

\maketitle

\begin{abstract}
\noindent
Surprisal, entropy, Kullback-Leibler divergence, mutual information.
\end{abstract}

\begin{figure}
  \centering
  \begin{tikzpicture}
\begin{axis}[
    width      = 8cm,
    height     = 6cm,
    axis lines = left,
    xlabel     = \(x\),
    ylabel     = {\(\log_{2} x\)},
]
%Below the red parabola is defined
\addplot [
    domain=-0:4,
    samples=50,
    very thick,
    color=mygray2,
    ]
    {log2(x)};

\addplot[
    mark=none,
    dotted,
    domain=-0:4,
    black,
    samples=2,
    ] {0};

\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    width      = 8cm,
    height     = 6cm,
    axis lines = left,
    xlabel = \(P(x)\),
    ylabel = {\(\text{Surprisal}( x ) \)},
]
%Below the red parabola is defined
\addplot [
    domain=-0:1,
    samples=50,
    very thick,
    color=mygray2,
    ]
    {-log2(x)};

\addplot[
    mark=none,
    dotted,
    domain=-0:1,
    black,
    samples=2,
    ] {0};

\end{axis}
\end{tikzpicture}

\caption{Logarithm and surprisal (to base 2).}
\label{fig:log-surprisal}
\end{figure}

\section{Information content (surprisal)}

Let $X$ be a random variable with support $\mathcal{X}$. For event $x \in \mathcal{X}$, the \emph{information content} $I_{X}(x)$ of $x$ (a.k.a.~\emph{surprisal} of $x$) under random variable $X$ is defined as:
\begin{align*}
  I_{X}(x) = - \log_{2} P(X = x)
\end{align*}
Intuitively speaking, the information content $I_{X}(x)$ is a measure of how surprised an agent with beliefs $X$ is (alternatively: how much the agent learns) when they observe $x$.

\section{Entropy}

The \emph{entropy} $\mathcal{H}(X)$ of a random variable is the expected information content under the assumption that the true distribution is $X$:\sidenote{Here and below, writing ``true distribution'' or similar formulations does not necessarily entail a strong commitment to actual truth. It is shorthand for more careful but cumbersome language like ``the distribution used as a reference or baseline which we assume to be true or treat as-if true.'' }
\begin{align*}
  \mathcal{H}(X) & = \sum_{x \in \mathcal{X}} P(X = x) \ I_{X}(x) \\
                 & = - \sum_{x \in \mathcal{X}} P(X = x) \ \log P(X = x)
\end{align*}
Intuitively speaking, the entropy $\mathcal{H}(X)$ measures the expected (or average) surprisal of an agent whose beliefs are $X$ when the true distribution is $X$.

\begin{itemize}
  \item example
  \item joint and conditional entropy
\end{itemize}

\section{Cross entropy}

The \emph{cross entropy} $\mathcal{H}(X,Y)$ of random variables $X$ and $Y$ measures the expectation of information content given $Y$ from the point of view of (assumed true) distribution $X$:
\begin{align*}
  \mathcal{H}(X,Y) & = \sum_{x \in \mathcal{X}} P(X = x) \ I_{Y}(x) \\
                   & = - \sum_{x \in \mathcal{X}} P(X = x) \ \log P(Y = x)
\end{align*}
Intuitively speaking, the cross entropy $\mathcal{H}(X,Y)$ measures the expected (or average) surprisal of an agent whose beliefs are $Y$ when the true distribution is $X$.

\section{Kullback-Leibler divergence (relative entropy)}

The \emph{Kullback-Leibler (KL) divergence} (also known as \emph{relative entropy}) measures the expected (or average) difference in information content between the distribution $Y$ and the true distribution $X$:
\begin{align*}
  D_{KL}(X || Y) & = \sum_{x \in \mathcal{X}} P(X = x) \ \left ( I_{Y}(x) - I_{X}(x) \right ) \\
  & = \sum_{x \in \mathcal{X}} P(X = x) \ \log  \frac{P(X=x)}{P(Y=x)}
\end{align*}
Intuitively speaking, the KL-divergence $D_{KL}(X || Y)$ measures how much more surprised an agent is, on average, when they hold beliefs described by $Y$ instead of the true distribution $X$.

KL-divergence $D_{KL}(X || Y)$ can be equivalently written in terms of the entropy $\mathcal{H}(X)$ of $X$ and the cross entropy $\mathcal{H}(X,Y)$ of $X$ and $Y$:
\begin{align*}
  D_{KL}(X || Y) = \mathcal{H}(X,Y) - \mathcal{H}(X)
\end{align*}

\begin{itemize}
  \item examples
  \item not a metric
\end{itemize}


\section{Mutual information}

\begin{align*}
  I(X;Y) & = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(Z)
\end{align*}

\end{document}

